Real-time-stock-market

****************pipeline of the project***************

Data Source (Stock Market API)
Live stock data is fetched from an external API.

Kafka (Streaming Layer)
Kafka streams the real-time stock data reliably and at scale.

S3 (Data Lake / Storage)
The streamed data is stored in S3 as raw data for durability and reprocessing.

Snowflake (Data Warehouse)
Data from S3 is loaded into Snowflake and organized into layers:

Raw: original, unprocessed data

Cleaned: validated and cleaned data

Business Ready: analytics-ready tables

dbt (Transformations)
dbt transforms data inside Snowflake (raw → cleaned → business-ready) using SQL and models.

Airflow (Orchestration)
Airflow schedules and coordinates the entire pipeline (ingestion, loading, transformations).

Power BI (Visualization)
Business-ready data from Snowflake is used to build dashboards and reports.

Docker
Used to containerize tools (Kafka, Airflow, dbt) for consistent deployment.

****************setup of the project******************

1. installed docker, python and created acc in snowflake
2. install python virtual environment and setup a folder with that environment. cmd - python -m venv venv
3. then in the main folder create the folder named infra which is for infrastructure.
4. docker initialization - so in this we have kept a docker-compose file (already given by YT), in that file we have certain libraries for docker to install once we run that file the system will automatically install the libraries/tools like kafka zookeeper, airflow, minio (alternate to aws s3) listed in that file basically in git we did git install and it installed all the depedencies mentioned in the file like that.
5. cmd for above is docker compose up -d. keep docker running in the background to execute this cmd or it will throw an error.
6. now docker has all the tools. all the tools will be running in the infra in docker except for airflow we have to initialize it. the cmds are 1. docker compose exec airflow-scheduler airflow db init 2. docker compose run airflow-webserver airflow db migrate.
7. in the docker compose file you can see that we have tools installed in that one of the tool is minio which is alternate to aws s3 and it has password and username mentioned inside.
8. after that we will setup the password for airflow via cmd. the cmd for that is docker compose exec airflow-webserver airflow users create --username Swayam --firstname Swayam --lastname Badhe --role Admin --email swbadhe@syr.edu --password Smvemjsunp@108.
9. in the docker now we have all the tools rrunning. to access kafka we will need to use thhe userinterface of kafka which is kafdrop. for minio we have to put username and password like mentioned in yml file.
10. kafka's UI is kafdrop (no id password), minio is itself (id password shared in yml), airflow's UI is airflow-webserver (id password created in cmd step 8).
11. now docker has been initialized. all the requird tools has been installed and running. now lets start with the structure of the project.

**************installing python tools to connect to the tools in kafka******************

1. first is api taken from finnhub.io. then we will install the python tools required to connect python to all the setup tools we created above. those tools are mentioned in requirements.txt file.
2. activate the venv by going into venv/scripts - activate. 
2. run the requirements file in the cmd. pip install -r requirements.txt. all the tools mentioned in this file will get installed..
3. so tools for the docker we installed in part 1. now we initilized the virtual env of python and installed their tools to connect to the tools in docker. such as boto3 to connect to minio etc.

**********************data streaming api to kafka (producer)***********************
4. now create the producer file which calls the api finnhub and streams the data into kafka.
5. now the data streaming to kafka hsa been completed.

*****************data storage minio. kafka to minio (consumer)**********************************
in this we are creating consumer file where we are sending the streaned data from kfka to store it into minio.
1. note that minio takes data in json and kafka in bytes. so before transfering awlways convert the type of the data.

*****************uploading data from minio to snowflake******************************
1. create the database and schema and under that schema create a table in which we are gonna load the bronze data ie. unceaned data.
2. create a file name minio_to_snoflake python file to transfer the data to the snowflake.
**Short README explanation (copy-paste ready):**

1. Initially, MinIO received fewer objects because files were being overwritten.
2. The consumer used second-level timestamps (`time.time()`) as filenames, so multiple Kafka messages arriving within the same second produced the same object key. MinIO silently overwrote earlier files.
3. Changing the filename to use millisecond precision (`time.time() * 1000`) made every object key unique, preventing overwrites and allowing all Kafka messages to be stored and later loaded into Snowflake.

***********************setup dbt airflow************************************
1. create new folder dbt_stocks in main folder. go to the python venv and in dbt_stocks folder type cmd "dbt init dbt_stocks".
2. after that it will ask you what db you are using type 1 for snowflake as it will be given in the option. then add the details related to the authentication. devname, paswword, dev role = ACCOUNTADMIN, db, wh, SCHEMA, threads = 4.
3. whenever we create dbt init name we are establishing a connection with the database in the snowflake from our python venv. so db init create certain folders like models on its own. and in that model we keep our stages folders containing sql files. so sql files are connected directly with the snowflake
3. create 3 folders in dbt_stocks>models> names are bronze, silver, gold.
4. we wont be using star schema in out project becuase its already 1 type of data or it belongs to one entity. so no need to do it
5. we have 3 diff models. bronze that is we are fetching the data from bronze_stock_quotes_raw they both are different. then in silver we are cleaning the data like rounding off, null incidents etc.
6. in gold model we generated the business ready data. also in gold model we have created views like candlesticks and treechart. so that all the workdon can we done in warehouse instead of power bi. so all the kpi are generated in warehouse.
7. now go to cmd venv and in dbt_stocks > dbt_stocks run "dbt run". it will run all the sql files.

**************************power bi setup******************************
1. open power bi  then get data and database then search snowflake and add credentials and then connect.
2. you will see the our databas. select the gold models sql file. since we have to only deal with them since they are business ready data. and in the connection setting we are going for direct query instead of import. since we have dynamic data. and the data will be constantly changing so direct query will work fine over here. now start doing Visualization.